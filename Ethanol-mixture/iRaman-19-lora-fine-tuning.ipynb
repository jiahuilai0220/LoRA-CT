{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import third-party packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import loralib as lora\n",
    "from model import CNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import optuna\n",
    "import json\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error, max_error\n",
    "import matplotlib.pyplot as plt\n",
    "# Determine the equipment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning of LoRA-CT method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 18:49:26,367] A new study created in memory with name: no-name-3e156ea8-baa3-4a74-a870-50947bbea87e\n",
      "[I 2025-02-17 18:49:28,042] Trial 0 finished with value: 0.8833972523476914 and parameters: {'Learning rate': 1e-05, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 0 with value: 0.8833972523476914.\n",
      "[I 2025-02-17 18:49:28,650] Trial 1 finished with value: 0.9841667003016153 and parameters: {'Learning rate': 0.01, 'Regularization coefficient': 0.01, 'Lora rank': 4}. Best is trial 1 with value: 0.9841667003016153.\n",
      "[I 2025-02-17 18:49:29,259] Trial 2 finished with value: 0.9669273589153252 and parameters: {'Learning rate': 0.01, 'Regularization coefficient': 0.01, 'Lora rank': 6}. Best is trial 1 with value: 0.9841667003016153.\n",
      "[I 2025-02-17 18:49:29,897] Trial 3 finished with value: 0.9071921525174123 and parameters: {'Learning rate': 1e-05, 'Regularization coefficient': 0.001, 'Lora rank': 8}. Best is trial 1 with value: 0.9841667003016153.\n",
      "[I 2025-02-17 18:49:30,537] Trial 4 finished with value: 0.9958510696448575 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 2}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:31,179] Trial 5 finished with value: 0.9956282322832557 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.1, 'Lora rank': 8}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:31,791] Trial 6 finished with value: 0.9955072551243319 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.01, 'Lora rank': 4}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:32,411] Trial 7 finished with value: 0.9955072551243319 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.01, 'Lora rank': 4}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:33,023] Trial 8 finished with value: 0.9955072898181065 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 4}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:33,638] Trial 9 finished with value: 0.984974092607485 and parameters: {'Learning rate': 0.01, 'Regularization coefficient': 0.1, 'Lora rank': 4}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:34,267] Trial 10 finished with value: 0.9862363562633663 and parameters: {'Learning rate': 0.0001, 'Regularization coefficient': 0.001, 'Lora rank': 2}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:34,886] Trial 11 finished with value: 0.9956282322832557 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.1, 'Lora rank': 8}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:35,477] Trial 12 finished with value: 0.9958484181791998 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.1, 'Lora rank': 2}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:36,085] Trial 13 finished with value: 0.9862351261968321 and parameters: {'Learning rate': 0.0001, 'Regularization coefficient': 0.1, 'Lora rank': 2}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:36,691] Trial 14 finished with value: 0.9958484181791998 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.1, 'Lora rank': 2}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:37,292] Trial 15 finished with value: 0.9958510696448575 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 2}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:37,902] Trial 16 finished with value: 0.9958510696448575 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 2}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:38,505] Trial 17 finished with value: 0.38888194368855544 and parameters: {'Learning rate': 1e-05, 'Regularization coefficient': 0.001, 'Lora rank': 2}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:39,112] Trial 18 finished with value: 0.9862363562633663 and parameters: {'Learning rate': 0.0001, 'Regularization coefficient': 0.001, 'Lora rank': 2}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:39,697] Trial 19 finished with value: 0.9958510696448575 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 2}. Best is trial 4 with value: 0.9958510696448575.\n",
      "[I 2025-02-17 18:49:40,309] Trial 20 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:40,942] Trial 21 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:41,543] Trial 22 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:42,170] Trial 23 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:42,815] Trial 24 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:43,472] Trial 25 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:44,104] Trial 26 finished with value: 0.8833972523476914 and parameters: {'Learning rate': 1e-05, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:44,739] Trial 27 finished with value: 0.9839671406921674 and parameters: {'Learning rate': 0.01, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:45,348] Trial 28 finished with value: 0.9853528139217196 and parameters: {'Learning rate': 0.0001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:45,986] Trial 29 finished with value: 0.8833972523476914 and parameters: {'Learning rate': 1e-05, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:46,649] Trial 30 finished with value: 0.9959636203217084 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.01, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:47,286] Trial 31 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:47,937] Trial 32 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:48,564] Trial 33 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:49,226] Trial 34 finished with value: 0.9839671406921674 and parameters: {'Learning rate': 0.01, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:49,886] Trial 35 finished with value: 0.9959636203217084 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.01, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:50,507] Trial 36 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:51,140] Trial 37 finished with value: 0.9956323721312222 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 8}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:51,763] Trial 38 finished with value: 0.9669273589153252 and parameters: {'Learning rate': 0.01, 'Regularization coefficient': 0.01, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:52,395] Trial 39 finished with value: 0.8833972523476914 and parameters: {'Learning rate': 1e-05, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:53,085] Trial 40 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:53,721] Trial 41 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:54,359] Trial 42 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:54,976] Trial 43 finished with value: 0.9959643078804646 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:55,587] Trial 44 finished with value: 0.9955072898181065 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 4}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:56,243] Trial 45 finished with value: 0.9956323721312222 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 8}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:56,862] Trial 46 finished with value: 0.9959636203217084 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.01, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:57,493] Trial 47 finished with value: 0.9853562973904256 and parameters: {'Learning rate': 0.0001, 'Regularization coefficient': 0.1, 'Lora rank': 6}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:58,092] Trial 48 finished with value: 0.9955072898181065 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 4}. Best is trial 20 with value: 0.9959643078804646.\n",
      "[I 2025-02-17 18:49:58,698] Trial 49 finished with value: 0.9956323721312222 and parameters: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 8}. Best is trial 20 with value: 0.9959643078804646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyperparameter results: {'Learning rate': 0.001, 'Regularization coefficient': 0.001, 'Lora rank': 6}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Modifying the primary model architecture\n",
    "def get_peft_model(base_model, lora_rank, lora_alpha):\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    class PeftModel(nn.Module):\n",
    "        def __init__(self, base_model, lora_rank, lora_alpha):\n",
    "            super(PeftModel, self).__init__()\n",
    "            for name, module in base_model.named_children():\n",
    "                if name != 'Linear':\n",
    "                    self.add_module(name, module)\n",
    "                else:\n",
    "                    self.add_module(name, lora.Linear(base_model.Linear.in_features, base_model.Linear.out_features, r=lora_rank, lora_alpha=lora_alpha, merge_weights=True))\n",
    "            \n",
    "        def forward(self, x):\n",
    "            for module in self.children():\n",
    "                x = module(x)\n",
    "            return x.squeeze(dim=1)\n",
    "    \n",
    "    return PeftModel(base_model, lora_rank, lora_alpha)\n",
    "\n",
    "def opt_model(trial):\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # Hyperparameter definitions\n",
    "    learning_rate = trial.suggest_categorical(\"Learning rate\", [1e-5, 1e-4, 1e-3, 1e-2])\n",
    "    weight_decay = trial.suggest_categorical(\"Regularization coefficient\", [1e-3, 1e-2, 1e-1])\n",
    "    lora_rank = trial.suggest_categorical(\"Lora rank\", [2, 4, 6, 8])\n",
    "    lora_alpha = lora_rank \n",
    "\n",
    "    X_train = np.load('ethanol-iRaman-data\\\\iRaman-19_spectra_train.npy')\n",
    "    y_train= np.load('ethanol-iRaman-data\\\\iRaman-ethanol_concentrations_train.npy')\n",
    "    \n",
    "    X_val = np.load('ethanol-iRaman-data\\\\iRaman-19_spectra_val.npy')\n",
    "    y_val= np.load('ethanol-iRaman-data\\\\iRaman-ethanol_concentrations_val.npy')\n",
    "    \n",
    "    X_test = np.load('ethanol-iRaman-data\\\\iRaman-19_spectra_test.npy')\n",
    "    y_test= np.load('ethanol-iRaman-data\\\\iRaman-ethanol_concentrations_test.npy')\n",
    "    X_train, X_left, y_train, y_left = train_test_split(X_train, y_train, test_size=0.99, random_state=84)\n",
    "    \n",
    "    \n",
    "    # Convert data to tensor and make sure it's on the right device\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32, device=device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=len(y_train), shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(y_val), shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "    # Load pre-trained model\n",
    "    model_state_dict = torch.load('ethanol-best_model_cnn_x1.pt', map_location=device, weights_only=True)\n",
    "    base_model = CNN()\n",
    "    peft_model = get_peft_model(base_model, lora_rank, lora_alpha).to(device)\n",
    "    peft_model.load_state_dict(model_state_dict, strict=False)\n",
    "    \n",
    "    # Perform weight decay\n",
    "    original_weight = peft_model.Linear.weight.data\n",
    "    decayed_weight = original_weight * 0.5\n",
    "    peft_model.Linear.weight.data = decayed_weight\n",
    "    #Confirm the layers to be trained\n",
    "    lora.mark_only_lora_as_trainable(peft_model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(peft_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    loss_fn = nn.SmoothL1Loss().to(device)\n",
    "    epochs = 100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        peft_model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.unsqueeze(dim=1).to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = peft_model(batch_x)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "    peft_model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x = batch_x.unsqueeze(dim=1).to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            output = peft_model(batch_x)\n",
    "            predictions.extend(output.cpu().tolist())\n",
    "            actuals.extend(batch_y.cpu().tolist())\n",
    "\n",
    "    if len(predictions) >= 2:\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "    return r2\n",
    "\n",
    "# hyperparameters tuning\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(opt_model, n_trials=50)\n",
    "print('Optimal hyperparameter results:', study.best_params)\n",
    "\n",
    "# Save the best hyperparameters\n",
    "best_params = study.best_params\n",
    "with open('iRaman-19-lora_peft_best_hyperparameters_1%.json', 'w') as f:\n",
    "    json.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the optimal combination of hyperparameters of the LoRA-CT method for fine-tuned training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.08234536647796631, Val Loss: 0.03668767958879471\n",
      "Epoch 2, Train Loss: 0.06154864281415939, Val Loss: 0.029870811849832535\n",
      "Epoch 3, Train Loss: 0.03291306272149086, Val Loss: 0.003986397758126259\n",
      "Epoch 4, Train Loss: 0.0034395521506667137, Val Loss: 0.007177548948675394\n",
      "Epoch 5, Train Loss: 0.010921959765255451, Val Loss: 0.007831976749002934\n",
      "Epoch 6, Train Loss: 0.011829303577542305, Val Loss: 0.0013270825147628784\n",
      "Epoch 7, Train Loss: 0.0028179665096104145, Val Loss: 0.004499609116464853\n",
      "Epoch 8, Train Loss: 0.006735605653375387, Val Loss: 0.00627336697652936\n",
      "Epoch 9, Train Loss: 0.008236842229962349, Val Loss: 0.0007008832762949169\n",
      "Epoch 10, Train Loss: 0.0012292726896703243, Val Loss: 0.002414921298623085\n",
      "Epoch 11, Train Loss: 0.004069401416927576, Val Loss: 0.00384300434961915\n",
      "Epoch 12, Train Loss: 0.005947343073785305, Val Loss: 0.0012637058971449733\n",
      "Epoch 13, Train Loss: 0.0019331497605890036, Val Loss: 0.00214054761454463\n",
      "Epoch 14, Train Loss: 0.0011379022616893053, Val Loss: 0.006414069794118404\n",
      "Epoch 15, Train Loss: 0.004802888259291649, Val Loss: 0.003565429477021098\n",
      "Epoch 16, Train Loss: 0.0021841442212462425, Val Loss: 0.001053834450431168\n",
      "Epoch 17, Train Loss: 0.0007875320152379572, Val Loss: 0.002319293562322855\n",
      "Epoch 18, Train Loss: 0.0030160073656588793, Val Loss: 0.0020097503438591957\n",
      "Epoch 19, Train Loss: 0.002526935888454318, Val Loss: 0.0007131334277801216\n",
      "Epoch 20, Train Loss: 0.00036283323424868286, Val Loss: 0.0020330825354903936\n",
      "Epoch 21, Train Loss: 0.001132941571995616, Val Loss: 0.0030292163137346506\n",
      "Epoch 22, Train Loss: 0.0021438580006361008, Val Loss: 0.0012539600720629096\n",
      "Epoch 23, Train Loss: 0.0006285964045673609, Val Loss: 0.0008265371434390545\n",
      "Epoch 24, Train Loss: 0.0005885537248104811, Val Loss: 0.0016658322419971228\n",
      "Epoch 25, Train Loss: 0.0016585325356572866, Val Loss: 0.0013699622359126806\n",
      "Epoch 26, Train Loss: 0.0012334927450865507, Val Loss: 0.000719708448741585\n",
      "Epoch 27, Train Loss: 0.0002964403829537332, Val Loss: 0.001383786671794951\n",
      "Epoch 28, Train Loss: 0.0008125646272674203, Val Loss: 0.001703934045508504\n",
      "Epoch 29, Train Loss: 0.00111493025906384, Val Loss: 0.0008062997949309647\n",
      "Epoch 30, Train Loss: 0.0002754565794020891, Val Loss: 0.0007154414779506624\n",
      "Epoch 31, Train Loss: 0.0003536066797096282, Val Loss: 0.0010952675947919488\n",
      "Epoch 32, Train Loss: 0.0008552989456802607, Val Loss: 0.0007934965542517602\n",
      "Epoch 33, Train Loss: 0.0004963637911714613, Val Loss: 0.0006089099915698171\n",
      "Epoch 34, Train Loss: 0.00019024091307073832, Val Loss: 0.0010536119807511568\n",
      "Epoch 35, Train Loss: 0.000599940656684339, Val Loss: 0.0008936477825045586\n",
      "Epoch 36, Train Loss: 0.0004827404045499861, Val Loss: 0.00042428571032360196\n",
      "Epoch 37, Train Loss: 0.00010177976218983531, Val Loss: 0.0005485569126904011\n",
      "Epoch 38, Train Loss: 0.00032435826142318547, Val Loss: 0.0005900213727727532\n",
      "Epoch 39, Train Loss: 0.00039398495573550463, Val Loss: 0.0003275763592682779\n",
      "Epoch 40, Train Loss: 0.00010190553439315408, Val Loss: 0.0004095887125004083\n",
      "Epoch 41, Train Loss: 0.00017931302136275917, Val Loss: 0.0005491271731443703\n",
      "Epoch 42, Train Loss: 0.0003408046322874725, Val Loss: 0.00031167446286417544\n",
      "Epoch 43, Train Loss: 0.00012253788008820266, Val Loss: 0.00026617778348736465\n",
      "Epoch 44, Train Loss: 0.00011553908552741632, Val Loss: 0.0003579635522328317\n",
      "Epoch 45, Train Loss: 0.00024289368593599647, Val Loss: 0.00022565701510757208\n",
      "Epoch 46, Train Loss: 0.00010336510604247451, Val Loss: 0.00020125923037994653\n",
      "Epoch 47, Train Loss: 5.207727735978551e-05, Val Loss: 0.0003381579590495676\n",
      "Epoch 48, Train Loss: 0.00017678349104244262, Val Loss: 0.0002417420328129083\n",
      "Epoch 49, Train Loss: 9.380307892570272e-05, Val Loss: 0.00015779650129843503\n",
      "Epoch 50, Train Loss: 5.1027967856498435e-05, Val Loss: 0.00021475805260706693\n",
      "Epoch 51, Train Loss: 0.00014134461525827646, Val Loss: 0.00016679374675732106\n",
      "Epoch 52, Train Loss: 8.217705908464268e-05, Val Loss: 0.0001489270362071693\n",
      "Epoch 53, Train Loss: 3.282899342593737e-05, Val Loss: 0.00022625054407399148\n",
      "Epoch 54, Train Loss: 9.774560749065131e-05, Val Loss: 0.00017180104623548687\n",
      "Epoch 55, Train Loss: 5.324145604390651e-05, Val Loss: 0.00012165307271061465\n",
      "Epoch 56, Train Loss: 2.4599674361525103e-05, Val Loss: 0.0001600147516001016\n",
      "Epoch 57, Train Loss: 7.65532095101662e-05, Val Loss: 0.00013647695595864207\n",
      "Epoch 58, Train Loss: 4.4116954086348414e-05, Val Loss: 0.00013286240573506802\n",
      "Epoch 59, Train Loss: 2.302768552908674e-05, Val Loss: 0.0001771412353264168\n",
      "Epoch 60, Train Loss: 5.82325883442536e-05, Val Loss: 0.00014332940918393433\n",
      "Epoch 61, Train Loss: 2.5773802917683497e-05, Val Loss: 0.00012261152733117342\n",
      "Epoch 62, Train Loss: 1.5817706298548728e-05, Val Loss: 0.00014129259216133505\n",
      "Epoch 63, Train Loss: 4.102270759176463e-05, Val Loss: 0.00012689207505900413\n",
      "Epoch 64, Train Loss: 1.650182639423292e-05, Val Loss: 0.00014273803390096873\n",
      "Epoch 65, Train Loss: 1.7035661585396156e-05, Val Loss: 0.00016500147467013448\n",
      "Epoch 66, Train Loss: 3.333641143399291e-05, Val Loss: 0.00013667553139384836\n",
      "Epoch 67, Train Loss: 1.0853043022507336e-05, Val Loss: 0.0001318577560596168\n",
      "Epoch 68, Train Loss: 1.611862171557732e-05, Val Loss: 0.00013640506949741393\n",
      "Epoch 69, Train Loss: 2.2110585632617585e-05, Val Loss: 0.00012846094614360482\n",
      "Epoch 70, Train Loss: 5.617932856694097e-06, Val Loss: 0.0001451771386200562\n",
      "Epoch 71, Train Loss: 1.5230079043249134e-05, Val Loss: 0.00014543769066222012\n",
      "Epoch 72, Train Loss: 1.5657575204386376e-05, Val Loss: 0.00012933614198118448\n",
      "Epoch 73, Train Loss: 5.772580152552109e-06, Val Loss: 0.00013214997306931764\n",
      "Epoch 74, Train Loss: 1.4756067685084417e-05, Val Loss: 0.00012805903679691255\n",
      "Epoch 75, Train Loss: 9.26856773730833e-06, Val Loss: 0.00013027740351390094\n",
      "Epoch 76, Train Loss: 4.717613592220005e-06, Val Loss: 0.000140608855872415\n",
      "Epoch 77, Train Loss: 1.1288462701486424e-05, Val Loss: 0.00013014463183935732\n",
      "Epoch 78, Train Loss: 4.707043444796e-06, Val Loss: 0.00012334111670497805\n",
      "Epoch 79, Train Loss: 5.966718163108453e-06, Val Loss: 0.0001232349022757262\n",
      "Epoch 80, Train Loss: 8.769897249294445e-06, Val Loss: 0.00012263447570148855\n",
      "Epoch 81, Train Loss: 3.0769663226237753e-06, Val Loss: 0.000131056978716515\n",
      "Epoch 82, Train Loss: 6.213457709236536e-06, Val Loss: 0.00012842821888625622\n",
      "Epoch 83, Train Loss: 5.0174485295428894e-06, Val Loss: 0.00011900366371264681\n",
      "Epoch 84, Train Loss: 2.352331193833379e-06, Val Loss: 0.00011773088772315532\n",
      "Epoch 85, Train Loss: 5.644211341859773e-06, Val Loss: 0.000117533425509464\n",
      "Epoch 86, Train Loss: 2.9676066333195195e-06, Val Loss: 0.00012323872942943126\n",
      "Epoch 87, Train Loss: 3.309013209218392e-06, Val Loss: 0.00012580311158671975\n",
      "Epoch 88, Train Loss: 4.413806436787127e-06, Val Loss: 0.00011935018846997991\n",
      "Epoch 89, Train Loss: 1.7847834214990144e-06, Val Loss: 0.00011698984599206597\n",
      "Epoch 90, Train Loss: 3.3829451240308117e-06, Val Loss: 0.00011767748947022483\n",
      "Epoch 91, Train Loss: 2.508334546291735e-06, Val Loss: 0.00012240624346304685\n",
      "Epoch 92, Train Loss: 1.7804454728320707e-06, Val Loss: 0.00012658193008974195\n",
      "Epoch 93, Train Loss: 3.0696905923832674e-06, Val Loss: 0.00012275706103537232\n",
      "Epoch 94, Train Loss: 1.5069226719788276e-06, Val Loss: 0.00011985644232481718\n",
      "Epoch 95, Train Loss: 2.1010237105656415e-06, Val Loss: 0.00012029643403366208\n",
      "Epoch 96, Train Loss: 1.9156475445925025e-06, Val Loss: 0.0001236368261743337\n",
      "Epoch 97, Train Loss: 1.056974838320457e-06, Val Loss: 0.00012717812205664814\n",
      "Epoch 98, Train Loss: 1.8987569774253643e-06, Val Loss: 0.00012502275058068335\n",
      "Epoch 99, Train Loss: 1.055794541571231e-06, Val Loss: 0.00012267172860447317\n",
      "Epoch 100, Train Loss: 1.3106663345752168e-06, Val Loss: 0.0001230324705829844\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set evaluation results:\n",
      "R2: 0.9999327177664968, RMSEP: 0.0018472589267390287, MAE: 0.0015965523198246956, MAX_ERROR: 0.0031269192695617676\n",
      "Validation set evaluation results:\n",
      "R2: 0.9959643566641277, RMSEP: 0.01568645740942922, MAE: 0.01100422473003467, MAX_ERROR: 0.04738231748342514\n",
      "Test set evaluation results:\n",
      "R2: 0.9946978110327538, RMSEP: 0.015856329540421498, MAE: 0.01148322205990553, MAX_ERROR: 0.04350316524505615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32, device=device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=len(y_train), shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(y_val), shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def get_peft_model(base_model, lora_rank, lora_alpha):\n",
    "    class PeftModel(nn.Module):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        def __init__(self, base_model, lora_rank, lora_alpha):\n",
    "            super(PeftModel, self).__init__()\n",
    "            for name, module in base_model.named_children():\n",
    "                if name != 'Linear':\n",
    "                    self.add_module(name, module)\n",
    "                else:\n",
    "                    self.add_module(name, lora.Linear(base_model.Linear.in_features, base_model.Linear.out_features, r=lora_rank, lora_alpha=lora_alpha, merge_weights=True))\n",
    "            \n",
    "        def forward(self, x):\n",
    "            for module in self.children():\n",
    "                x = module(x)\n",
    "            return x.squeeze(dim=1)\n",
    "    \n",
    "    return PeftModel(base_model, lora_rank, lora_alpha)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, learning_rate, weight_decay):\n",
    "    model.to(device)\n",
    "    loss_fn = nn.SmoothL1Loss().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    epochs = 100\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.unsqueeze(dim=1).to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.unsqueeze(dim=1).to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                output = model(batch_x)\n",
    "                loss = loss_fn(output, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def plot_learning_curve(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.unsqueeze(dim=1).to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            output = model(batch_x)\n",
    "            predictions.extend(output.cpu().tolist())\n",
    "            actuals.extend(batch_y.cpu().tolist())\n",
    "    final_r2 = r2_score(actuals, predictions)\n",
    "    RMSEP = root_mean_squared_error(actuals, predictions)\n",
    "    MAE = mean_absolute_error(actuals, predictions)\n",
    "    MAX_ERROR = max_error(actuals, predictions)\n",
    "    return final_r2, RMSEP, MAE, MAX_ERROR\n",
    "\n",
    "def save_model(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open('iRaman-19-lora_peft_best_hyperparameters_1%.json', 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    \n",
    "    learning_rate = best_params['Learning rate']\n",
    "    weight_decay = best_params['Regularization coefficient']\n",
    "    lora_rank = int(best_params['Lora rank'])\n",
    "    lora_alpha = lora_rank\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    X_train = np.load('ethanol-iRaman-data\\\\iRaman-19_spectra_train.npy')\n",
    "    y_train= np.load('ethanol-iRaman-data\\\\iRaman-ethanol_concentrations_train.npy')\n",
    "    \n",
    "    X_val = np.load('ethanol-iRaman-data\\\\iRaman-19_spectra_val.npy')\n",
    "    y_val= np.load('ethanol-iRaman-data\\\\iRaman-ethanol_concentrations_val.npy')\n",
    "    \n",
    "    X_test = np.load('ethanol-iRaman-data\\\\iRaman-19_spectra_test.npy')\n",
    "    y_test= np.load('ethanol-iRaman-data\\\\iRaman-ethanol_concentrations_test.npy')\n",
    "    X_train, X_left, y_train, y_left = train_test_split(X_train, y_train, test_size=0.99, random_state=84)\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    base_model = CNN()\n",
    "    peft_model = get_peft_model(base_model, lora_rank, lora_alpha)\n",
    "\n",
    "    model_state_dict = torch.load('ethanol-best_model_cnn_x1.pt', map_location=device, weights_only=True)\n",
    "    peft_model.load_state_dict(model_state_dict, strict=False)\n",
    "    # Perform weight decay\n",
    "    original_weight = peft_model.Linear.weight.data\n",
    "    decayed_weight = original_weight * 0.5\n",
    "    peft_model.Linear.weight.data = decayed_weight\n",
    "\n",
    "    lora.mark_only_lora_as_trainable(peft_model)\n",
    "\n",
    "    peft_model = peft_model.to(device)\n",
    "    train_losses, val_losses = train_model(peft_model, train_loader, val_loader,learning_rate, weight_decay)\n",
    "    plot_learning_curve(train_losses, val_losses)\n",
    "\n",
    "    print('Training set evaluation results:')\n",
    "    r2, rmse, mae, me = evaluate_model(peft_model, train_loader, device)\n",
    "    print(f'R2: {r2}, RMSEP: {rmse}, MAE: {mae}, MAX_ERROR: {me}')\n",
    "\n",
    "    print('Validation set evaluation results:')\n",
    "    r2, rmse, mae, me = evaluate_model(peft_model, val_loader, device)\n",
    "    print(f'R2: {r2}, RMSEP: {rmse}, MAE: {mae}, MAX_ERROR: {me}')\n",
    "\n",
    "    print('Test set evaluation results:')\n",
    "    r2, rmse, mae, me = evaluate_model(peft_model, test_loader, device)\n",
    "    print(f'R2: {r2}, RMSEP: {rmse}, MAE: {mae}, MAX_ERROR: {me}')\n",
    "    \n",
    "    # Save the weights of the lora module\n",
    "    torch.save(lora.lora_state_dict(peft_model), 'iRaman-19-lora_state_dict_1%.pt')\n",
    "    save_dict = {}\n",
    "    # Save the weights of the fine-tuned BN layer\n",
    "    model_state = peft_model.state_dict()\n",
    "    for k,v in model_state.items():\n",
    "        if 'BatchNorm' in k:\n",
    "            save_dict[k] = v\n",
    "    torch.save(save_dict, 'iRaman-19-peft_model_BN_1%.pt')\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
